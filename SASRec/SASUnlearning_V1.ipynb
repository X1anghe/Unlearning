{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from model import SASRec\n",
    "from utils import *\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.dataset = 'ml-1m-incremental'\n",
    "args.train_dir='default'\n",
    "args.batch_size = 128\n",
    "args.one_batch = 1\n",
    "args.lr = 0.0003\n",
    "args.maxlen = 200\n",
    "args.hidden_units = 50\n",
    "args.num_blocks = 2\n",
    "args.num_epochs = 201\n",
    "args.num_heads = 1\n",
    "args.dropout_rate = 0.5\n",
    "args.l2_emb = 0.0\n",
    "args.device = 'cuda'\n",
    "args.inference_only = False\n",
    "args.state_dict_path = 'ml-1m_default/SASRec.epoch=201.lr=0.001.layer=2.head=1.hidden=50.maxlen=200.pth'\n",
    "args.incremental_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(fgt_users, dataset):\n",
    "    [user_train, user_valid, user_test, usernum, itemnum] = copy.deepcopy(dataset)\n",
    "    fgt_dataset = [{}, {}, {}, 0 ,0]\n",
    "    remain_dataset = [{}, {}, {}, 0 ,0]\n",
    "\n",
    "    fgt_items = set()\n",
    "    remain_items = set()\n",
    "    for u in user_train.keys():\n",
    "        if (u in fgt_users) and (u not in fgt_dataset[0].keys()):\n",
    "            fgt_dataset[0][u] = user_train[u]\n",
    "            fgt_dataset[1][u] = user_valid[u]\n",
    "            fgt_dataset[2][u] = user_test[u]\n",
    "            fgt_items.update(fgt_dataset[0][u])\n",
    "        else:\n",
    "            remain_dataset[0][u] = user_train[u]\n",
    "            remain_dataset[1][u] = user_valid[u]\n",
    "            remain_dataset[2][u] = user_test[u]\n",
    "            remain_items.update(remain_dataset[0][u])\n",
    "\n",
    "    fgt_dataset[3] = len(fgt_dataset[0].keys())\n",
    "    fgt_dataset[4] = len(fgt_items)\n",
    "\n",
    "    remain_dataset[3] = len(remain_dataset[0].keys())\n",
    "    remain_dataset[4] = len(remain_items)\n",
    "\n",
    "    cc = 0.0\n",
    "    for u in dataset[0]:\n",
    "        cc += len(dataset[0][u])\n",
    "    print('average sequence length old_dataset: %.2f' % (cc / len(dataset[0])))\n",
    "\n",
    "    cc = 0.0\n",
    "    for u in fgt_dataset[0]:\n",
    "        cc += len(fgt_dataset[0][u])\n",
    "    print('average sequence length fgt_dataset: %.2f' % (cc / len(fgt_dataset[0])))\n",
    "\n",
    "    cc = 0.0\n",
    "    for u in remain_dataset[0]:\n",
    "        cc += len(remain_dataset[0][u])\n",
    "    print('average sequence length remain_dataset: %.2f' % (cc / len(remain_dataset[0])))\n",
    "\n",
    "    return fgt_dataset, remain_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(args, _dataset):\n",
    "    dataset = copy.deepcopy(_dataset)\n",
    "    model = SASRec(dataset[3], dataset[4], args).to(args.device) # no ReLU activation in original SASRec implementation?\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        try:\n",
    "            torch.nn.init.xavier_normal_(param.data)\n",
    "        except:\n",
    "            pass # just ignore those failed init layers\n",
    "    \n",
    "    # this fails embedding init 'Embedding' object has no attribute 'dim'\n",
    "    # model.apply(torch.nn.init.xavier_uniform_)\n",
    "    \n",
    "    model.train() # enable model training\n",
    "    \n",
    "    epoch_start_idx = 1\n",
    "    if args.state_dict_path is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(args.state_dict_path, map_location=torch.device(args.device)))\n",
    "            tail = args.state_dict_path[args.state_dict_path.find('epoch=') + 6:]\n",
    "            epoch_start_idx = int(tail[:tail.find('.')]) + 1\n",
    "        except: # in case your pytorch version is not 1.6 etc., pls debug by pdb if load weights failed\n",
    "            print('failed loading state_dicts, pls check file path: ', end=\"\")\n",
    "            print(args.state_dict_path)\n",
    "            print('pdb enabled for your quick check, pls type exit() if you do not need it')\n",
    "            import pdb; pdb.set_trace()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateeee(model, data, name):\n",
    "    model.eval()\n",
    "    t_test = evaluate(model, data, args)\n",
    "    print(name + ' test (NDCG@10: %.4f, HR@10: %.4f)' % (t_test[0], t_test[1]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(pretrained_model, current_model, sampler):\n",
    "    \"\"\"\n",
    "    sampler = Norlmal Sampler\n",
    "    \"\"\"\n",
    "    u, seq, pos, neg = sampler.next_batch() # tuples to ndarray\n",
    "    u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "    cur_pos_logits, cur_neg_logits = current_model(u, seq, pos, neg)\n",
    "    pre_pos_logits, pre_neg_logits = pretrained_model(u, seq, pos, neg)\n",
    "\n",
    "\n",
    "    # P: pretrained model; Q: current model.\n",
    "    prob_p = torch.nn.functional.softmax(pre_pos_logits, dim=-1)\n",
    "    prob_q = torch.nn.functional.softmax(cur_pos_logits, dim=-1)\n",
    "\n",
    "    log_prob_q = torch.log(prob_q + 1e-12)\n",
    "    kl_loss = torch.nn.functional.kl_div(log_prob_q, prob_p, reduction='batchmean')\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_loss(current_model, sampler, optimizer, lossfunc):\n",
    "    \"\"\"\n",
    "    sampler = Unlearning Sampler\n",
    "    \"\"\"\n",
    "    u, seq, pos, neg = sampler.next_batch() # tuples to ndarray\n",
    "    u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "    pos_logits, neg_logits = current_model(u, seq, pos, neg)\n",
    "    pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)\n",
    "    # print(\"\\neye ball check raw_logits:\"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0\n",
    "    optimizer.zero_grad()\n",
    "    indices = np.where(pos != 0)\n",
    "    loss = lossfunc(pos_logits[indices], pos_labels[indices])\n",
    "    loss += lossfunc(neg_logits[indices], neg_labels[indices])\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_neq(l, r, s):  \n",
    "    t = np.random.randint(l, r)\n",
    "    while t in s:\n",
    "        t = np.random.randint(l, r)\n",
    "    return t\n",
    "\n",
    "class tSimpleWarpSampler(object):\n",
    "    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10):\n",
    "        self.User = User\n",
    "        self.usernum = usernum\n",
    "        self.itemnum = itemnum\n",
    "        self.batch_size = batch_size\n",
    "        self.maxlen = maxlen\n",
    "        self.UserList = list(User.keys())\n",
    "\n",
    "    def tsample(self):\n",
    "        user = np.random.choice(self.UserList)\n",
    "        while len(self.User[user]) <= 1: user = np.random.choice(self.UserList)\n",
    "        \n",
    "        seq = np.zeros([self.maxlen], dtype=np.int32)\n",
    "        pos = np.zeros([self.maxlen], dtype=np.int32)\n",
    "        neg = np.zeros([self.maxlen], dtype=np.int32)\n",
    "        nxt = self.User[user][-1]\n",
    "        idx = self.maxlen - 1\n",
    "\n",
    "        ts = set(self.User[user])\n",
    "        for i in reversed(self.User[user][:-1]):\n",
    "            seq[idx] = i\n",
    "            pos[idx] = nxt\n",
    "            if nxt != 0: neg[idx] = random_neq(1, self.itemnum + 1, ts)\n",
    "            nxt = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        return (user, seq, pos, neg)\n",
    "\n",
    "    def next_batch(self):\n",
    "        one_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            one_batch.append(self.tsample())\n",
    "        \n",
    "        return zip(*one_batch)\n",
    "\n",
    "def get_sampler(dataset, n_batch):\n",
    "    [train, valid, test, usernum, itemnum] = dataset\n",
    "    num_batch = len(train) // n_batch # tail? + ((len(user_train) % args.batch_size) != 0)\n",
    "    sampler = tSimpleWarpSampler(train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen)\n",
    "    return num_batch, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearning(_max_steps, _unlerning_sampler, _remain_sampler, _pre_model, _cur_model):\n",
    "\n",
    "    epoch_start_idx = 1\n",
    "    bce_criterion = torch.nn.BCEWithLogitsLoss() # torch.nn.BCELoss()\n",
    "    adam_optimizer = torch.optim.Adam(_cur_model.parameters(), lr=args.lr, betas=(0.9, 0.98))\n",
    "\n",
    "    T = 0.0\n",
    "    t0 = time.time()\n",
    "\n",
    "    max_steps = _max_steps\n",
    "    _normal_loss = 0.0\n",
    "\n",
    "    bad_weight = 0.5\n",
    "\n",
    "    # for epoch in range(epoch_start_idx, args.incremental_epochs + 1):\n",
    "    move_step = 0\n",
    "    while move_step < max_steps: # tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\n",
    "        _kl_loss = kl_loss(_pre_model, _cur_model, _remain_sampler)\n",
    "        _normal_loss = normal_loss(_cur_model, _unlerning_sampler, adam_optimizer, bce_criterion)\n",
    "        loss = bad_weight * _kl_loss - bad_weight * _normal_loss\n",
    "        # for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)\n",
    "        loss.backward()\n",
    "        \n",
    "        adam_optimizer.step()\n",
    "        move_step += 1\n",
    "        # print(\"loss in epoch {} iteration {}: {}\".format(epoch_start_idx, move_step, loss.item())) # expected 0.4~0.6 after init few epochs\n",
    "            \n",
    "    print(\"loss in step {}: {}\".format(move_step, -loss.item())) # expected 0.4~0.6 after init few epochs\n",
    "    print(\"Done\")\n",
    "    return _pre_model, _cur_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset = data_partition('ml-1m') # {train}, {valid}, {test}, usernum, itemnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SASRec(\n",
       "  (item_emb): Embedding(3417, 50, padding_idx=0)\n",
       "  (pos_emb): Embedding(200, 50)\n",
       "  (emb_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (attention_layernorms): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
       "  )\n",
       "  (attention_layers): ModuleList(\n",
       "    (0-1): 2 x MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (forward_layernorms): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
       "  )\n",
       "  (forward_layers): ModuleList(\n",
       "    (0-1): 2 x PointWiseFeedForward(\n",
       "      (conv1): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
       "      (dropout1): Dropout(p=0.5, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
       "      (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (last_layernorm): LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_model = model_init(args, old_dataset)\n",
    "cur_model = model_init(args, old_dataset)\n",
    "cur_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sequence length old_dataset: 163.50\n",
      "average sequence length fgt_dataset: 162.00\n",
      "average sequence length remain_dataset: 163.50\n"
     ]
    }
   ],
   "source": [
    "fgt_users = [5,100,500]\n",
    "fgt_dataset, remain_dataset = data_preparation(fgt_users, old_dataset)\n",
    "fgt_num_batch, fgt_sampler = get_sampler(fgt_dataset, args.one_batch)\n",
    "remain_num_batch, remain_sampler = get_sampler(remain_dataset, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................\n",
      " cur_old  test (NDCG@10: 0.5888, HR@10: 0.8219)\n",
      "\n",
      " cur_fgt  test (NDCG@10: 0.8333, HR@10: 1.0000)\n",
      "............................................................\n",
      " cur_remain  test (NDCG@10: 0.5859, HR@10: 0.8196)\n"
     ]
    }
   ],
   "source": [
    "evaluateeee(cur_model, old_dataset, '\\n cur_old ')\n",
    "evaluateeee(cur_model, fgt_dataset, '\\n cur_fgt ')\n",
    "evaluateeee(cur_model, remain_dataset, '\\n cur_remain ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in step 60: 2.124324321746826\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "pre_model, cur_model = unlearning(60, fgt_sampler, remain_sampler, pre_model, cur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................\n",
      " cur_old  test (NDCG@10: 0.4688, HR@10: 0.7124)\n",
      "\n",
      " cur_fgt  test (NDCG@10: 0.0000, HR@10: 0.0000)\n",
      "............................................................\n",
      " cur_remain  test (NDCG@10: 0.4716, HR@10: 0.7207)\n",
      "............................................................\n",
      " pre_old  test (NDCG@10: 0.5932, HR@10: 0.8263)\n",
      "\n",
      " pre_fgt  test (NDCG@10: 1.0000, HR@10: 1.0000)\n",
      "............................................................\n",
      " pre_remain  test (NDCG@10: 0.5909, HR@10: 0.8219)\n"
     ]
    }
   ],
   "source": [
    "evaluateeee(cur_model, old_dataset, '\\n cur_old ')\n",
    "evaluateeee(cur_model, fgt_dataset, '\\n cur_fgt ')\n",
    "evaluateeee(cur_model, remain_dataset, '\\n cur_remain ')\n",
    "evaluateeee(pre_model, old_dataset, '\\n pre_old ')\n",
    "evaluateeee(pre_model, fgt_dataset, '\\n pre_fgt ')\n",
    "evaluateeee(pre_model, remain_dataset, '\\n pre_remain ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
